<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on John Edwards</title>
    <link>https://johnbedwards.io/tags/r/</link>
    <description>Recent content in R on John Edwards</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© 2021</copyright>
    <lastBuildDate>Mon, 17 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://johnbedwards.io/tags/r/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>March Madness 2025 First Round Projections</title>
      <link>https://johnbedwards.io/blog/march_madness_2025_round_one/</link>
      <pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/march_madness_2025_round_one/</guid>
      <description>Introduction and methodology Following up on my Kaggle March Mania top 40 finishfrom last year (in addition to my top 30 finish from 2021), I&amp;rsquo;m excited to share my Mach Madness projections for this year! I want to get a bit more granular this year and be able to share some individual game projections as well, as an exercise in preparing these kinds of visuals (not to mention the social media clout).</description>
    </item>
    
    <item>
      <title>Predicting the Olympics</title>
      <link>https://johnbedwards.io/blog/olympic_prediction_contest/</link>
      <pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/olympic_prediction_contest/</guid>
      <description>Stade de France; Chabe01, CC BY-SA 4.0, via Wikimedia Commons
I had the pleasure of competing in the Royal Statistical Society&amp;rsquo;s 2024 Olympics prediction contest.I also had the pleasure of winning! While I feel that my code is too messy to share at present and I lack the motivation to clean it up for public consumption, I did want to discuss at a high level my approach and lessons learned from this competition.</description>
    </item>
    
    <item>
      <title>Hacking by stacking—how to get better {tidymodels} performance with {stacks}</title>
      <link>https://johnbedwards.io/blog/stacks/</link>
      <pubDate>Wed, 21 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/stacks/</guid>
      <description>Introduction There is no more frustrating feeling than finishing just shy of a podium position in a Kaggle competition. Those precious competition points were right there! If only you had just a slightly better log loss! Alas, you exhausted every tool in your data science toolkit, and that was the best you could do. Unless&amp;hellip; there was another way to get even better performance out of your models. Something so absurdly simple to implement that it felt almost like hacking the leaderboard.</description>
    </item>
    
    <item>
      <title>2021 March Madness Kaggle Solution</title>
      <link>https://johnbedwards.io/blog/march_madness_2021/</link>
      <pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/march_madness_2021/</guid>
      <description>Our approach is ensemble the shit out of everything. We will hold out the 2015-2019 games for validation purposes. We will prepare and optimize two sets of models - one, an ensemble of general team strength features trained on 1985-2014 games, and two - an ensemble of general team strength features + adjusted ratings based on box-score data trained on 2002-2014 games. Let&amp;rsquo;s prepare the first approach.
import sys!{sys.executable} -m pip install pandas sklearn numpy rpy2 trueskill catboost hyperopt rayimport numpy as npimport pandas as pdfrom sklearn.</description>
    </item>
    
  </channel>
</rss>
