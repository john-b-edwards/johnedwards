<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on John Edwards</title>
    <link>https://johnbedwards.io/tags/python/</link>
    <description>Recent content in python on John Edwards</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Â© 2021</copyright>
    <lastBuildDate>Sat, 18 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://johnbedwards.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Google&#39;s Data Science Agent is just a Kaggle slop generator</title>
      <link>https://johnbedwards.io/blog/google_data_sci_slop/</link>
      <pubDate>Sat, 18 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/google_data_sci_slop/</guid>
      <description>Google I/O 2019; Alexander Shcherbakov, CC BY-SA 3.0via Wikimedia Commons
Google I/O was this past week, and predictably, the focus was LLM-powered tools. Among the products Google rolled out was something that caught my eye&amp;ndash;an LLM powered &amp;ldquo;Data Science Agent&amp;rdquo; that would take in data and with prompting, break down a dataset, develop a plan of attack for approaching a data science problem, and even generate Colab notebooks.
Today we are launching Data Science Agent, a Google Labs experiment.</description>
    </item>
    
    <item>
      <title>2021 March Madness Kaggle Solution</title>
      <link>https://johnbedwards.io/blog/march_madness_2021/</link>
      <pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/march_madness_2021/</guid>
      <description>Our approach is ensemble the shit out of everything. We will hold out the 2015-2019 games for validation purposes. We will prepare and optimize two sets of models - one, an ensemble of general team strength features trained on 1985-2014 games, and two - an ensemble of general team strength features + adjusted ratings based on box-score data trained on 2002-2014 games. Let&amp;rsquo;s prepare the first approach.
import sys!{sys.executable} -m pip install pandas sklearn numpy rpy2 trueskill catboost hyperopt rayimport numpy as npimport pandas as pdfrom sklearn.</description>
    </item>
    
    <item>
      <title>Applying LRMC Rankings to College Football, Part Two</title>
      <link>https://johnbedwards.io/blog/lrmc_pt_2/</link>
      <pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/lrmc_pt_2/</guid>
      <description>The following was originally published on the CFBD Blogand has been reproduced here with edits for clarity.
This is the second and final part of a series on implementing LRMC rankings for CFB! This entry presupposes you are familiar with the mathematical concepts behind LRMC. To view part one, which covers these concepts, click here.
When we last left off, we covered how LRMC works and how to implement it mathematically.</description>
    </item>
    
  </channel>
</rss>
