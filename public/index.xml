<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>John Edwards</title>
    <link>https://johnbedwards.io/</link>
    <description>Recent content on John Edwards</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© 2021</copyright>
    <lastBuildDate>Mon, 18 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://johnbedwards.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Predicting the Olympics</title>
      <link>https://johnbedwards.io/blog/olympic_prediction_contest/</link>
      <pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/olympic_prediction_contest/</guid>
      <description>Stade de France; Chabe01, CC BY-SA 4.0 , via Wikimedia Commons
I had the pleasure of competing in the Royal Statistical Society&amp;rsquo;s 2024 Olympics prediction contest. I also had the pleasure of winning! While I feel that my code is too messy to share at present and I lack the motivation to clean it up for public consumption, I did want to discuss at a high level my approach and lessons learned from this competition.</description>
    </item>
    
    <item>
      <title>Google&#39;s Data Science Agent is just a Kaggle slop generator</title>
      <link>https://johnbedwards.io/blog/google_data_sci_slop/</link>
      <pubDate>Sat, 18 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/google_data_sci_slop/</guid>
      <description>Google I/O 2019; Alexander Shcherbakov, CC BY-SA 3.0 via Wikimedia Commons
Google I/O was this past week, and predictably, the focus was LLM-powered tools. Among the products Google rolled out was something that caught my eye&amp;ndash;an LLM powered &amp;ldquo;Data Science Agent&amp;rdquo; that would take in data and with prompting, break down a dataset, develop a plan of attack for approaching a data science problem, and even generate Colab notebooks.
Today we are launching Data Science Agent, a Google Labs experiment.</description>
    </item>
    
    <item>
      <title>Animating Plays in Julia with Makie.jl</title>
      <link>https://johnbedwards.io/blog/big-data-bowl-makie/</link>
      <pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/big-data-bowl-makie/</guid>
      <description>Introduction No matter what kind of project you want to tackle, you&amp;rsquo;re going to want the ability to understand what&amp;rsquo;s happening on a given play with the Big Data Bowl dataset. Easier said than done! You can go to YouTube and try to scrub through hours of film to see plays, but 1) that takes a ton of time and 2) means interfacing with the game in a meaningful way beyond manipulating a spreadsheet, which no self-respecting analytics nerd would ever do,,,</description>
    </item>
    
    <item>
      <title>Hacking by stacking—how to get better {tidymodels} performance with {stacks}</title>
      <link>https://johnbedwards.io/blog/stacks/</link>
      <pubDate>Wed, 21 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/stacks/</guid>
      <description>Introduction There is no more frustrating feeling than finishing just shy of a podium position in a Kaggle competition. Those precious competition points were right there! If only you had just a slightly better log loss! Alas, you exhausted every tool in your data science toolkit, and that was the best you could do. Unless&amp;hellip; there was another way to get even better performance out of your models. Something so absurdly simple to implement that it felt almost like hacking the leaderboard.</description>
    </item>
    
    <item>
      <title>Using Flux.jl to model Scrabble turns</title>
      <link>https://johnbedwards.io/blog/predicting-scrabble-point-values/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/predicting-scrabble-point-values/</guid>
      <description>I recently participated in a Kaggle community competition where the objective was to predict the point value of a word played on the 20th turn of a Scrabble game given a dataset of Scrabble games played on Woogles.io , an online Scrabble website. After learning a lot about neural networks and Julia, I managed to swing first place! Below is my write-up of my solution, which also represents my first serious foray into working with Julia.</description>
    </item>
    
    <item>
      <title>Lessons from picking the 2022 CFB Season</title>
      <link>https://johnbedwards.io/blog/picking_2022/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/picking_2022/</guid>
      <description>As we reach the end of the 2021-22 college football season (the national championship game is unfolding on my TV as I type this), I wanted to take a look back at my performance in the College Football Data predictions contest . Minimum 400 games picked (not counting the NCG, but it should not affect my performance) (I picked 726 games total, for reference), I finished:
 1st in straight-up picks 3rd in ATS picks 1st in absolute error 4th in mean squared error  So I did pretty well, especially for someone who picked as many games as I did!</description>
    </item>
    
    <item>
      <title>2021 March Madness Kaggle Solution</title>
      <link>https://johnbedwards.io/blog/march_madness_2021/</link>
      <pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/march_madness_2021/</guid>
      <description>Our approach is ensemble the shit out of everything. We will hold out the 2015-2019 games for validation purposes. We will prepare and optimize two sets of models - one, an ensemble of general team strength features trained on 1985-2014 games, and two - an ensemble of general team strength features + adjusted ratings based on box-score data trained on 2002-2014 games. Let&amp;rsquo;s prepare the first approach.
import sys!{sys.executable} -m pip install pandas sklearn numpy rpy2 trueskill catboost hyperopt rayimport numpy as npimport pandas as pdfrom sklearn.</description>
    </item>
    
    <item>
      <title>Applying LRMC Rankings to College Football, Part Two</title>
      <link>https://johnbedwards.io/blog/lrmc_pt_2/</link>
      <pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/lrmc_pt_2/</guid>
      <description>The following was originally published on the CFBD Blog and has been reproduced here with edits for clarity.
This is the second and final part of a series on implementing LRMC rankings for CFB! This entry presupposes you are familiar with the mathematical concepts behind LRMC. To view part one, which covers these concepts, click here.
When we last left off, we covered how LRMC works and how to implement it mathematically.</description>
    </item>
    
    <item>
      <title>Applying LRMC Rankings to College Football, Part One</title>
      <link>https://johnbedwards.io/blog/lrmc_pt_1/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/blog/lrmc_pt_1/</guid>
      <description>The following was originally published on the CFBD Blog and has been reproduced here with edits for clarity.
As Bill covered earlier this season , calculating strength-of-schedule adjusted metrics like SRS are a little tricky given how few non-conference games teams play this year. While I think his technique for conference-based SRS is a great attempt at a really difficult problem, I think there is some value in discussing alternative approaches to evaluating team strength this year that do not struggle with the same singularity issues as SRS.</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://johnbedwards.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/about/</guid>
      <description>I am a sports analyst/data scientist from Virginia Beach, VA, currently residing in Seattle, WA. I received my BS in Literature, Media, and Communications from Georgia Tech in 2019. I have previously worked as a contributing writer/analyst for a variety of websites, including The Athletic, and Sporting News, but have since moved to work in front offices, interning with the Baltimore Orioles before landing with the Seattle Mariners. I have served as an analyst and lead data scientist of pitching before landing in my current position as manager of data science for the team.</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>https://johnbedwards.io/projects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/projects/</guid>
      <description>NFLData.jl Developed and released an open source Julia package allowing users to easily work with nflverse data. You can view the documentation for the package here .
Royal Statistical Society 2024 Olympic Prediction Contest Finished 1st overall in the RSS&amp;rsquo;s 2024 Olympic Prediction contest. While my code is not available, I have done a high-level writeup of my solution and you can view it here. Kaggle 2024 March Mania Contest Finished 33rd overall in Kaggle&amp;rsquo;s 2024 March Mania contest using using team-strength methods and some minor gaming of the competition.</description>
    </item>
    
    <item>
      <title>Resume</title>
      <link>https://johnbedwards.io/resume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://johnbedwards.io/resume/</guid>
      <description>Proficiencies  R  catboost data.table ggplot shiny lme4 mgcv rvest rselenium tensorflow tidyverse tidymodels xgboost   Python  catboost django numpy pandas sklearn tensorflow xgboost   Julia  Dataframes.jl Flux.jl Plots.jl StatsModels.jl MixedModels.jl XGBoost.jl   SQL HTML CSS Git  Experiences Manager of Data Science, Seattle Mariners 2024-Present I manage projects, mentor other data scientists, and develop machine learning models and tools for the Mariners baseball operations department.</description>
    </item>
    
  </channel>
</rss>
